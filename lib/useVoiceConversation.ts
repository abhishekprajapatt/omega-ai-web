'use client';import { useCallback, useRef, useState, useEffect } from 'react';import {  VoiceListener,  VoiceSynthesizer,  AudioProcessor,  TranscriptResult,  SpeechOptions,  getEmotionModulation,  convertSpeechToTextAPI,  convertTextToSpeechAPI,  audioBlobToBase64,} from '@/lib/voiceUtils';export interface UseVoiceConversationOptions {  onTranscript?: (transcript: string, isFinal: boolean) => void;  onError?: (error: string) => void;  onListeningStart?: () => void;  onListeningEnd?: () => void;  onSpeakingStart?: () => void;  onSpeakingEnd?: () => void;  onProcessingStart?: () => void;  language?: string;  voiceRate?: number;  voicePitch?: number;  voiceVolume?: number;  emotion?: 'happy' | 'sad' | 'angry' | 'calm' | 'excited' | 'neutral';  autoPlay?: boolean; }export interface VoiceConversationState {  isListening: boolean;  isSpeaking: boolean;  isProcessing: boolean;  transcript: string;  error: string | null;}export function useVoiceConversation(  options: UseVoiceConversationOptions = {},) {  const {    onTranscript,    onError,    onListeningStart,    onListeningEnd,    onSpeakingStart,    onSpeakingEnd,    onProcessingStart,    language = 'en-US',    voiceRate = 1.0,    voicePitch = 1.0,    voiceVolume = 0.85,    emotion = 'neutral',    autoPlay = true,  } = options;  const [state, setState] = useState<VoiceConversationState>(() => ({    isListening: false,    isSpeaking: false,    isProcessing: false,    transcript: '',    error: null,  }));  const listenerRef = useRef<VoiceListener | null>(null);  const synthesizerRef = useRef<VoiceSynthesizer | null>(null);  const audioProcessorRef = useRef<AudioProcessor | null>(null);  const micStreamRef = useRef<MediaStream | null>(null);  const silenceTimerRef = useRef<NodeJS.Timeout | null>(null);  const inactivityTimerRef = useRef<NodeJS.Timeout | null>(null);  const SILENCE_DURATION = 1500;  const INACTIVITY_TIMEOUT = 30000;   useEffect(() => {    if (VoiceListener.isSupported()) {      listenerRef.current = new VoiceListener({        language,        continuous: true,        interimResults: true,      });    }    if (VoiceSynthesizer.isSupported()) {      synthesizerRef.current = new VoiceSynthesizer({        language,        rate: voiceRate,        pitch: voicePitch,        volume: voiceVolume,      });    }    if (AudioProcessor.isSupported()) {      audioProcessorRef.current = new AudioProcessor();    }    return () => {      listenerRef.current?.abort();      synthesizerRef.current?.stop();      audioProcessorRef.current?.close();    };  }, []);  const updateState = useCallback(    (updates: Partial<VoiceConversationState>) => {      setState((prev) => ({ ...prev, ...updates }));    },    [],  );  const startListening = useCallback(async () => {    if (!listenerRef.current) {      const error = 'Voice listener not available';      updateState({ error });      onError?.(error);      return;    }    try {      micStreamRef.current = await navigator.mediaDevices.getUserMedia({        audio: true,      });      if (audioProcessorRef.current && micStreamRef.current) {        const audioContext =          (window.AudioContext || (window as any).webkitAudioContext) &&          new (window.AudioContext || (window as any).webkitAudioContext)();        if (audioContext && micStreamRef.current) {          const source = audioContext.createMediaStreamSource(            micStreamRef.current,          );          audioProcessorRef.current.setupVisualization(source);        }      }      updateState({ isListening: true, transcript: '', error: null });      onListeningStart?.();      listenerRef.current.startListening(        (result: TranscriptResult) => {          updateState({            transcript: result.text,            error: null,          });          onTranscript?.(result.text, result.isFinal);          if (silenceTimerRef.current) {            clearTimeout(silenceTimerRef.current);          }          if (result.isFinal) {            silenceTimerRef.current = setTimeout(() => {              stopListening();            }, SILENCE_DURATION);          }          if (inactivityTimerRef.current) {            clearTimeout(inactivityTimerRef.current);          }          inactivityTimerRef.current = setTimeout(() => {            stopListening();          }, INACTIVITY_TIMEOUT);        },        (error: string) => {          updateState({ error });          onError?.(error);        },      );    } catch (error) {      const errorMessage = `Microphone access denied: ${String(error)}`;      updateState({ error: errorMessage, isListening: false });      onError?.(errorMessage);    }  }, [updateState, onListeningStart, onTranscript, onError]);  const stopListening = useCallback(() => {    if (listenerRef.current) {      listenerRef.current.stopListening();      updateState({ isListening: false });      onListeningEnd?.();      if (silenceTimerRef.current) {        clearTimeout(silenceTimerRef.current);      }      if (inactivityTimerRef.current) {        clearTimeout(inactivityTimerRef.current);      }      if (micStreamRef.current) {        micStreamRef.current.getTracks().forEach((track) => track.stop());        micStreamRef.current = null;      }    }  }, [updateState, onListeningEnd]);  const playResponse = useCallback(    async (      text: string,      useServerTTS: boolean = false,      customOptions?: Partial<SpeechOptions>,    ) => {      if (!synthesizerRef.current && !useServerTTS) {        onError?.('Voice synthesizer not available');        return;      }      try {        onProcessingStart?.();        updateState({ isProcessing: true });        const emotionMod = getEmotionModulation(emotion);        if (useServerTTS) {          const result = await convertTextToSpeechAPI(            text,            language,            undefined,            'openai',          );          if (result.error || !result.audio) {            throw new Error(result.error || 'Failed to convert text to speech');          }          const audioUrl = URL.createObjectURL(result.audio);          const audio = new Audio(audioUrl);          updateState({ isSpeaking: true, isProcessing: false });          onSpeakingStart?.();          audio.onended = () => {            updateState({ isSpeaking: false });            onSpeakingEnd?.();            URL.revokeObjectURL(audioUrl);          };          audio.onerror = () => {            const errorMsg = 'Failed to play audio';            updateState({ isSpeaking: false, error: errorMsg });            onError?.(errorMsg);            URL.revokeObjectURL(audioUrl);          };          audio.play().catch((err) => {            const errorMsg = `Audio playback failed: ${String(err)}`;            updateState({ isSpeaking: false, error: errorMsg });            onError?.(errorMsg);          });        } else {          const options: SpeechOptions = {            text,            language,            rate: customOptions?.rate ?? voiceRate * (emotionMod.rate / 1.0),            pitch:              customOptions?.pitch ?? voicePitch * (emotionMod.pitch / 1.0),            volume:              customOptions?.volume ?? voiceVolume * (emotionMod.volume / 0.85),            ...customOptions,            onStart: () => {              updateState({ isSpeaking: true, isProcessing: false });              onSpeakingStart?.();            },            onEnd: () => {              updateState({ isSpeaking: false });              onSpeakingEnd?.();            },            onError: (error: string) => {              updateState({ isSpeaking: false, error });              onError?.(error);            },          };          if (synthesizerRef.current) {            await synthesizerRef.current.speak(options);          }        }      } catch (error) {        const errorMessage = `Failed to play response: ${String(error)}`;        updateState({          isProcessing: false,          isSpeaking: false,          error: errorMessage,        });        onError?.(errorMessage);      }    },    [      language,      voiceRate,      voicePitch,      voiceVolume,      emotion,      updateState,      onProcessingStart,      onSpeakingStart,      onSpeakingEnd,      onError,    ],  );  const stopSpeaking = useCallback(() => {    if (synthesizerRef.current) {      synthesizerRef.current.stop();      updateState({ isSpeaking: false });      onSpeakingEnd?.();    }  }, [updateState, onSpeakingEnd]);  const getTranscript = useCallback(() => {    return listenerRef.current?.getTranscript() || state.transcript;  }, [state.transcript]);  const clearTranscript = useCallback(() => {    if (listenerRef.current) {      listenerRef.current.resetTranscript();    }    updateState({ transcript: '' });  }, [updateState]);  const getAvailableVoices = useCallback(() => {    if (synthesizerRef.current) {      return synthesizerRef.current.getVoicesForLanguage(language);    }    return [];  }, [language]);  const setLanguage = useCallback((newLanguage: string) => {    if (listenerRef.current) {      listenerRef.current.setLanguage(newLanguage);    }    if (synthesizerRef.current) {      synthesizerRef.current.setLanguage(newLanguage);    }  }, []);  const setVoiceByName = useCallback((voiceName: string) => {    if (synthesizerRef.current) {      synthesizerRef.current.setVoiceByName(voiceName);    }  }, []);  const getAudioData = useCallback(() => {    if (audioProcessorRef.current) {      return {        frequency: audioProcessorRef.current.getFrequencyData(),        waveform: audioProcessorRef.current.getWaveformData(),        average: audioProcessorRef.current.getAverageFrequency(),      };    }    return null;  }, []);  const convertAudioToText = useCallback(    async (      audioBlob: Blob,      provider: 'openai' | 'google' | 'deepgram' = 'openai',    ) => {      try {        updateState({ isProcessing: true, error: null });        const result = await convertSpeechToTextAPI(          audioBlob,          language,          provider,        );        if (result.error) {          throw new Error(result.error);        }        updateState({          transcript: result.transcript,          isProcessing: false,          error: null,        });        onTranscript?.(result.transcript, true);        return result;      } catch (error) {        const errorMessage = `STT conversion failed: ${String(error)}`;        updateState({ error: errorMessage, isProcessing: false });        onError?.(errorMessage);        return { transcript: '', confidence: 0, error: errorMessage };      }    },    [language, updateState, onTranscript, onError],  );  const convertTextToSpeech = useCallback(    async (      text: string,      model: 'openai' | 'elevenlabs' | 'google' = 'openai',    ) => {      try {        if (!text || text.trim().length === 0) {          throw new Error('Text is required');        }        const result = await convertTextToSpeechAPI(          text,          language,          undefined,          model,        );        if (result.error || !result.audio) {          throw new Error(result.error || 'TTS conversion failed');        }        return result.audio;      } catch (error) {        const errorMessage = `Text-to-speech conversion failed: ${String(error)}`;        updateState({ error: errorMessage });        onError?.(errorMessage);        return null;      }    },    [language, updateState, onError],  );  return {    ...state,    startListening,    stopListening,    playResponse,    stopSpeaking,    clearTranscript,    getTranscript,    getAvailableVoices,    getAudioData,    convertAudioToText,    convertTextToSpeech,    setLanguage,    setVoiceByName,    isSupported: {      listener: VoiceListener.isSupported(),      synthesizer: VoiceSynthesizer.isSupported(),      audioProcessor: AudioProcessor.isSupported(),    },  };}export type VoiceConversationHook = ReturnType<typeof useVoiceConversation>;